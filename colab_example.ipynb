{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "colab_example.ipynb",
      "provenance": [],
      "authorship_tag": "ABX9TyN/uQxaXuBdk9EYkQmJlzQ+",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/mlellouch/contrastive-learning/blob/main/colab_example.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Contrastive For Class Detection\n",
        "We will train a siamese network with contrastive learning to learn how to detect if two images are of the same class. \n",
        "\n",
        "Please note the following:\n",
        "\n",
        "*   Google colab has an option to load a notebook from github, you should probably use that.\n",
        "*   This notebook will access your google drive data\n",
        "\n",
        "\n",
        "## Google Drive Setup:"
      ],
      "metadata": {
        "id": "_FaTFamsdYpJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/gdrive', force_remount=True)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rtc14MZ9eL4l",
        "outputId": "34475c4f-7036-4244-b325-aa1e7c43662f"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/gdrive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "\n",
        "project_dir = \"gdrive/MyDrive/cont_learning_example\"\n",
        "os.makedirs(project_dir, exist_ok=True)"
      ],
      "metadata": {
        "id": "XG4740rd5zFR"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Clone the repo, to access the data \n",
        "If you plan on using your own data, don't forget to upload it in some way. Some of your options are:\n",
        "\n",
        "1.   Upload from the colab interface\n",
        "2.   Write a cell that copies data from google drive to this dir. \n",
        "\n",
        "Anyhow, remember that:\n",
        "\n",
        "\n",
        "*   In each different run, the data in this dir will be deleted\n",
        "*   However, using data from the mounted google drive is very slow, thus it is better to load the data to this folder at the start of the run\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "0NzqoN3ze5NO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!git clone https://github.com/mlellouch/contrastive-learning.git"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qMaQmb65eztA",
        "outputId": "58450d9c-a31f-4376-89f3-683aa11d931c"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "fatal: destination path 'contrastive-learning' already exists and is not an empty directory.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# init python\n",
        "%reload_ext autoreload\n",
        "%autoreload 2\n",
        "%matplotlib inline\n",
        "import os\n",
        "import torch\n",
        "import csv\n",
        "import torch.nn as nn\n",
        "import numpy as np\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from torchvision import transforms, utils\n",
        "import torchvision.models as models\n",
        "import tqdm\n",
        "from PIL import Image\n",
        "import random"
      ],
      "metadata": {
        "id": "3GlLUAm1gImk"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# The Data\n",
        "This project expects data like in \"data/animals/train\" where all the images of the same class are in a single directory.\n",
        "\n",
        "# Prepare The Dataset\n",
        "The dataset class defines how the images will be read, and what their labels are going to be. \n",
        "\n",
        "In our case, we will each time load a pair of images, and label the pair with 1 if it's a pair of the same class, and 0 if not. \n"
      ],
      "metadata": {
        "id": "gYfXqbEbftuO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "BATCH_SIZE = 16\n",
        "\n",
        "class PairsDataset(Dataset):\n",
        "\n",
        "  def __init__(self, source_dir, transforms):\n",
        "    \"\"\"\n",
        "    source_dir: the initial dir where all the images are\n",
        "    transforms: a pytorch image transform. This is used if you want to \"enrich\" the data with new datapoints by using the normal data. For example, by cropping the image a bit, or mirroring it. \n",
        "    \"\"\"\n",
        "\n",
        "    self.source_dir = source_dir\n",
        "    self.transforms = transforms\n",
        "\n",
        "  def __len__(self):\n",
        "    return BATCH_SIZE\n",
        "\n",
        "\n",
        "  def process_images(self, *images):\n",
        "    \"\"\"\n",
        "    given a list of image paths, loads them, and passes them through the transform\n",
        "    \"\"\"\n",
        "\n",
        "    out_images = []\n",
        "    for image_path in images:\n",
        "      np_image = np.array(Image.open(image_path))\n",
        "      out_images.append(np_image.astype('float32'))\n",
        "      \n",
        "    return [self.transforms(Image.open(image_path)) for image_path in images]\n",
        "\n",
        "  def __getitem__(self, index):\n",
        "    \"\"\"\n",
        "    The function that is called when a new image is requested for training\n",
        "    \"\"\"\n",
        "    all_classes = os.listdir(self.source_dir)\n",
        "    # randomly select if we're going to return a same class image\n",
        "    should_get_same_class = random.randint(0,1) \n",
        "    base_class = random.choice(all_classes)\n",
        "    base_class_dir = os.path.join(self.source_dir, base_class)  \n",
        "    if should_get_same_class:\n",
        "      img1, img2 = random.sample(os.listdir(base_class_dir), 2)\n",
        "      img1, img2 = os.path.join(base_class_dir, img1), os.path.join(base_class_dir, img2)\n",
        "      return self.process_images(img1, img2), 1\n",
        "\n",
        "    else:\n",
        "      img1 = random.choice(os.listdir(base_class_dir))\n",
        "\n",
        "      # choose image from another class\n",
        "      all_classes.remove(base_class)\n",
        "      new_class = random.choice(all_classes)\n",
        "      new_class_dir = os.path.join(self.source_dir, new_class)\n",
        "      img2 = random.choice(os.listdir(new_class_dir))\n",
        "\n",
        "      img1, img2 = os.path.join(base_class_dir, img1), os.path.join(new_class_dir, img2)\n",
        "      return self.process_images(img1, img2), 0\n"
      ],
      "metadata": {
        "id": "lNY92JzofkOX"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Sanity test the dataset\n",
        "sanity_test_transforms = transforms.Compose([\n",
        "                                             transforms.Resize(400),\n",
        "                                             transforms.RandomCrop(360),\n",
        "                                             transforms.RandomHorizontalFlip(),\n",
        "                                             \n",
        "                                             transforms.ToTensor(),\n",
        "                                             transforms.Normalize(mean=[0.5, 0.5, 0.5],std=[0.5, 0.5, 0.5]),\n",
        "])\n",
        "sanity_test_dataset = PairsDataset('./contrastive-learning/data/animals/train', sanity_test_transforms)\n",
        "a = sanity_test_dataset[0]\n",
        "assert type(a[1]) == int\n",
        "assert list(a[0][0].shape) == [3, 360, 360]"
      ],
      "metadata": {
        "id": "9ND8HtJnn9Zq"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Define The Model\n",
        "This model will use as backbone a pretrained network, which will make our trainig time a lot shorter. If you plan on using it for unnatural images (like web-pages), this pretrained model will still help, but expect a longer training time\n",
        "\n",
        "## Contrastive Loss vs Classification Loss\n",
        "Reminder: In this method, the model will return a vector for each image. From this point, we have to options (which you can decide in the code):\n",
        "\n",
        "\n",
        "1.   **Contrastive Loss:** where we will train the model to output similar vectors for images of the same class. Then to classify a new pair, you need to calculate the distance between the two vectors, and if it's below some threshold, it is a pair of the same class. This might be useful if you intend on doing more interesting stuff with the vector than just classifying.\n",
        "2.   **Classification Loss:** from the two outputted vectors, we can train a new part of the network that learns to predict given the vectors, are they from the same class. \n",
        "\n"
      ],
      "metadata": {
        "id": "S_9eWNdms1n0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class SiameseNetwork(torch.nn.Module):\n",
        "\n",
        "  def __init__(self, backbone='resnet18', contrastive_loss=True):\n",
        "    super(SiameseNetwork, self).__init__()\n",
        "    self.contrastive_loss = contrastive_loss\n",
        "\n",
        "    available_backbones = {\n",
        "        'resnet18': models.resnet18,\n",
        "        'resnet34': models.resnet34,\n",
        "        'resnet50': models.resnet50\n",
        "    }\n",
        "\n",
        "    self.backbone = available_backbones[backbone](pretrained=True)\n",
        "\n",
        "    # the fully connected part, which will only be used if we're using classification loss\n",
        "    self.fc1 = nn.Sequential(\n",
        "        nn.Linear(2000, 256),\n",
        "        nn.ReLU(inplace=True),\n",
        "        \n",
        "        nn.Linear(256,1),\n",
        "        nn.Sigmoid()\n",
        "    )\n",
        "\n",
        "  def forward(self, image1, image2):\n",
        "    out1 = self.backbone(image1)\n",
        "    out2 = self.backbone(image2)\n",
        "    if self.contrastive_loss:\n",
        "      return out1, out2\n",
        "    else:\n",
        "      return self.fc1(torch.cat((out1, out2), 1))"
      ],
      "metadata": {
        "id": "ajx-2tU6ppJR"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Define the Criterions (Losses)\n",
        "\n"
      ],
      "metadata": {
        "id": "JExmm1n32k-z"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class ContrastiveLoss(torch.nn.Module):\n",
        "\n",
        "    def __init__(self, margin=1.0):\n",
        "        super(ContrastiveLoss, self).__init__()\n",
        "        self.margin = margin\n",
        "\n",
        "    def forward(self, output, y):\n",
        "        out1, out2 = output\n",
        "        diff = out1 - out2\n",
        "        dist_sq = torch.sum(torch.pow(diff, 2), 1)\n",
        "        dist = torch.sqrt(dist_sq)\n",
        "        mdist = self.margin - dist\n",
        "        dist = torch.clamp(mdist, min=0.0)\n",
        "        loss = y * dist_sq + (1 - y) * torch.pow(dist, 2)\n",
        "        loss = torch.sum(loss) / 2.0 / out1.size()[0]\n",
        "        return loss\n",
        "\n",
        "\n",
        "class BCELoss(torch.nn.Module):\n",
        "\n",
        "  def __init__(self):\n",
        "    super(BCELoss, self).__init__()\n",
        "    self.loss = torch.nn.BCELoss()\n",
        "\n",
        "  def forward(self, output, y):\n",
        "    output = output.reshape(y.shape)\n",
        "    y = y.type(torch.float32)\n",
        "    return self.loss(output, y)\n"
      ],
      "metadata": {
        "id": "IuJal8Y0362_"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Define The Training Loop"
      ],
      "metadata": {
        "id": "D2PA2Czwxgd9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def train(model, dataset, criterion, optimizer, epochs=10, run_name='test'):\n",
        "  # define the log:\n",
        "  log_file = os.path.join(project_dir, f'log_{run_name}.csv')\n",
        "  log = csv.DictWriter(open(log_file, 'w', newline=''), fieldnames=['epoch', 'loss'])\n",
        "  log.writeheader()\n",
        "\n",
        "  device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "  train_loader = torch.utils.data.DataLoader(dataset=dataset,\n",
        "                                              batch_size=BATCH_SIZE,\n",
        "                                              shuffle=True)\n",
        "\n",
        "\n",
        "  for epoch in tqdm.tqdm(range(epochs)):\n",
        "      for (img1_set, img2_set), labels in train_loader:\n",
        "        img1_set = img1_set.to(device)\n",
        "        img2_set = img2_set.to(device)\n",
        "      \n",
        "\n",
        "        # Forward + Backward + Optimize\n",
        "        optimizer.zero_grad()\n",
        "        output = model(img1_set, img2_set)\n",
        "        loss = criterion(output, labels)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "      log.writerow({'epoch': epoch, 'loss': loss.detach().item()})\n",
        "\n",
        "  \n",
        "  # Save the Trained Model\n",
        "  model_file_name = os.path.join(project_dir, f'trained_model_{run_name}.pth')\n",
        "  torch.save(model.state_dict(), model_file_name)\n",
        "  print(\"Saved model at {}\".format(model_file_name))\n",
        "  return model"
      ],
      "metadata": {
        "id": "g-eQpnZXxd-P"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Let's Test It"
      ],
      "metadata": {
        "id": "uGwpCNA72QLQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# this is a single experiment cell. You should copy and paste it if you want to toy with the parameters\n",
        "\n",
        "contrastive_loss = True\n",
        "if contrastive_loss:\n",
        "  criterion = ContrastiveLoss()\n",
        "else:\n",
        "  criterion = BCELoss()\n",
        "\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "model = SiameseNetwork(contrastive_loss=contrastive_loss)\n",
        "model.to(device)\n",
        "optimizer = torch.optim.Adam(model.parameters())\n",
        "\n",
        "run_transforms = transforms.Compose([\n",
        "  transforms.Resize(400),\n",
        "  transforms.RandomAffine(30),\n",
        "  transforms.RandomCrop(360),\n",
        "  transforms.RandomHorizontalFlip(),\n",
        "  \n",
        "  transforms.ToTensor(),\n",
        "  transforms.Normalize(mean=[0.5, 0.5, 0.5],std=[0.5, 0.5, 0.5]),\n",
        "])\n",
        "\n",
        "dataset = PairsDataset('./contrastive-learning/data/animals/train', run_transforms)\n",
        "\n",
        "model = train(model, dataset, criterion, optimizer, epochs=15)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PqJGY1N10Sug",
        "outputId": "2cd27c11-baef-4829-faa6-19faacf17d57"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 15/15 [05:29<00:00, 21.98s/it]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Saved model at gdrive/MyDrive/cont_learning_example/trained_model_test.pth\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "OUVYE4TK5qWE"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}